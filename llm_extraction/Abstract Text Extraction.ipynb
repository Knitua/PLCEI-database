{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excel Empty Row Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('All_Files.xlsx')\n",
    "df_cleaned = df.dropna(subset=['Abstract'])\n",
    "df_cleaned.to_excel('cleaned_file.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(filename):\n",
    "  client = OpenAI()\n",
    "\n",
    "  batch_input_file = client.files.create(\n",
    "    file=open(filename, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    "  )\n",
    "\n",
    "  batch_input_file_id = batch_input_file.id\n",
    "\n",
    "  batch = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\"\n",
    "  )\n",
    "  return batch\n",
    "\n",
    "def check_batch_status(batch):\n",
    "  client = OpenAI()\n",
    "  retrieve_batch = client.batches.retrieve(batch.id)\n",
    "  return retrieve_batch\n",
    "\n",
    "def download_output(retrieve_batch, filename):\n",
    "  client = OpenAI()\n",
    "  content = client.files.content(retrieve_batch.output_file_id)\n",
    "  with open(filename, \"wb\") as f:\n",
    "      f.write(content.content)\n",
    "      \n",
    "def get_answer_list(filepath):\n",
    "    content_list, custom_id_digits = [], []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                choices = data['response']['body']['choices']\n",
    "                for choice in choices:\n",
    "                    message = choice['message']\n",
    "                    if message['role'] == 'assistant':\n",
    "                        content = message['content']\n",
    "                        content_list.append(content)\n",
    "                        \n",
    "                        custom_id = int(data['custom_id'])\n",
    "                        custom_id_digits.append(custom_id)\n",
    "            except (KeyError, IndexError, ValueError):\n",
    "                continue\n",
    "    df = pd.DataFrame({'ID': custom_id_digits,'Answer': content_list}).sort_values(by='ID').reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_research = pd.read_excel('cleaned_file.xlsx', sheet_name='Sheet1')\n",
    "text_list = test_research['Abstract']\n",
    "file_path1 = 'judge2_abstract1'\n",
    "file_path2 = 'judge2_abstract2'\n",
    "file_path3 = 'judge2_abstract3'\n",
    "q1 = '''\n",
    "Respond in JSON format to each of the following questions only based on the text:\n",
    "1.Based on the information in the text,please determine whether it contains information on life cycle assessment(life-cycle assessment),LCA,environmental impact(problems),or calculation of carbon(greenhouse gas,GHG) emissions(footprint)?\n",
    "(If it contains relevant information, answer\"1\";if it has words that have the same meaning as\"LCA studies are expected\" or \"LCA studies will be carried out in the future\",answer\"0\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "2.Based on the information in the text,whether it contains the following words:resins or resin)?\n",
    "(If it contains resins or specific types of resins(such as PE,PP,PS,PVC,PET,ABS,PLA,etc.),answer \"1\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "3.Based on the information in the text,whether it contains polymers or polymer?\n",
    "(Do a word-by-word comparison of the text,if the word \"polymer\" or\"polymers\" appear(whether or not it's relevant to the topic), answer \"1\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "4.Based on the information in the text,whether it contains plastics(plastic) or waste?\n",
    "(Do a word-by-word comparison of the text,if the word “plastic”,\"plastics\",\"package\",\"packaging\" appear(whether or not it's relevant to the topic), answer \"1\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "Here is an example of the output format:\n",
    "{\n",
    "\"article_check1\": “1”,\n",
    "\"article_check2\": \"1\",\n",
    "\"article_check3\": \"0\",\n",
    "\"article_check4\": \"0\",\n",
    "}\n",
    "'''\n",
    "q2='''\n",
    "Respond in JSON format to each of the following questions only based on the text:\n",
    "1.Based on the information in the text,please determine whether it contains information on polymer resins made into fiber products?\n",
    "(If it contains resin synthetic fibers(such as polypropylene synthetic fibers,etc.), answer\"1\";if it contains information about fiber-reinforced polymers (such as glass fiber reinforced polymers(GFRP),carbon fiber reinforced polymer(CFRP), fiber reinforced polymer(FRP), etc.)or resin matrix,answer\"0\";if it contains carbon fiber(fibers) or glass fiber(fibers) and resin(resins) in one text,answer\"0\";if it contains plastic waste,recycling(circular) and fibers at the same time,answers\"0\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "2.Based on the information in the text,please determine whether it contains information on lignin,cellulose or metal polymers(metal-polymer)?\n",
    "(If it contains lignin,cellulose or metal polymers,and contains terms ('resins'(such as PE,PP,PS,PVC,PET,ABS,PLA,etc.) ,or 'plastics'), answer\"0\";If it contains lignin,cellulose or metal polymers and does not mention resins or plastics, answer\"1\";if it does not contain lignin,cellulose,metal polymers,or you are not sure, answer \"0\")\n",
    "Here is an example of the output format:\n",
    "{\n",
    "\"article_check1\":\"0\",\n",
    "\"article_check2\":\"0\",\n",
    "}\n",
    "'''\n",
    "q3='''\n",
    "Respond in JSON format to each of the following questions only based on the text:\n",
    "1.Based on the information in the text,please determine whether it contains information on resins from trees (such as pine resin or pine chemical products)?\n",
    "(If it contains resins from trees (such as pine resin or pine chemical products), answer\"1\";if it contains electrode resin(resins),answer\"0\";if it does not contain relevant information or you are not sure, answer \"0\")\n",
    "Here is an example of the output format:\n",
    "{\n",
    "\"article_check1:\"0\",\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Invocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6714b8a4c85881908825d66cd0d284bc', completion_window='24h', created_at=1729411236, endpoint='/v1/chat/completions', input_file_id='file-IikAsKlN9swTMMZ2mgCaJ1NX', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1729497636, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "Batch(id='batch_6714b8d247348190a983da2310bec21d', completion_window='24h', created_at=1729411282, endpoint='/v1/chat/completions', input_file_id='file-IiqNkN2K82f5V5vFhlwLj4Cy', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1729497682, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "Batch(id='batch_6714b8e4114c8190bddbc56ec389d307', completion_window='24h', created_at=1729411300, endpoint='/v1/chat/completions', input_file_id='file-eGFp4uyD7wSdHdDjr8ur5iSF', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1729497700, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "request_list1 = []\n",
    "request_list2 = []\n",
    "request_list3 = []\n",
    "for i, text in enumerate(text_list): \n",
    "    request1 = {\"custom_id\": str(i), \"method\": \"POST\", \"url\": \"/v1/chat/completions\"}\n",
    "    body1 = {\"model\": \"gpt-4-turbo\", \"temperature\": 0, \"response_format\": {\"type\": \"json_object\"}}\n",
    "    body1['messages'] = [\n",
    "        {\"role\": \"system\", \"content\": q1},\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    request1['body'] = body1\n",
    "    request_list1.append(request1)\n",
    "\n",
    "    request2 = {\"custom_id\": str(i), \"method\": \"POST\", \"url\": \"/v1/chat/completions\"}\n",
    "    body2 = {\"model\": \"gpt-4-turbo\", \"temperature\": 0, \"response_format\": {\"type\": \"json_object\"}}\n",
    "    body2['messages'] = [\n",
    "        {\"role\": \"system\", \"content\": q2},\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    request2['body'] = body2\n",
    "    request_list2.append(request2)\n",
    "\n",
    "    request3 = {\"custom_id\": str(i), \"method\": \"POST\", \"url\": \"/v1/chat/completions\"}\n",
    "    body3 = {\"model\": \"gpt-4-turbo\", \"temperature\": 0, \"response_format\": {\"type\": \"json_object\"}}\n",
    "    body3['messages'] = [\n",
    "        {\"role\": \"system\", \"content\": q3},\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    request3['body'] = body3\n",
    "    request_list3.append(request3)\n",
    "    \n",
    "with open(file_path1 + '.jsonl', 'w') as file:\n",
    "    for entry in request_list1:\n",
    "        json_line = json.dumps(entry)\n",
    "        file.write(json_line + '\\n')\n",
    "        \n",
    "batch1 = send_request(file_path1 + '.jsonl')\n",
    "print(batch1)\n",
    "\n",
    "with open(file_path2 + '.jsonl', 'w') as file:\n",
    "    for entry in request_list2:\n",
    "        json_line = json.dumps(entry)\n",
    "        file.write(json_line + '\\n')\n",
    "\n",
    "batch2 = send_request(file_path2 + '.jsonl')\n",
    "print(batch2)\n",
    "\n",
    "with open(file_path3 + '.jsonl', 'w') as file:\n",
    "    for entry in request_list3:\n",
    "        json_line = json.dumps(entry)\n",
    "        file.write(json_line + '\\n')\n",
    "\n",
    "batch3 = send_request(file_path3 + '.jsonl')\n",
    "print(batch3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Batch Task Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchRequestCounts(completed=2396, failed=0, total=2410)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_batch1 = check_batch_status(batch1)\n",
    "retrieve_batch1.request_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchRequestCounts(completed=2410, failed=0, total=2410)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_batch2 = check_batch_status(batch2)\n",
    "retrieve_batch2.request_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchRequestCounts(completed=2400, failed=0, total=2410)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_batch3 = check_batch_status(batch3)\n",
    "retrieve_batch3.request_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return Task Results and Save as JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(retrieve_batch.output_file_id)\n",
    "download_output(retrieve_batch1, file_path1 + \"_output.jsonl\")\n",
    "answer_df1 = get_answer_list(file_path1 + \"_output.jsonl\")\n",
    "download_output(retrieve_batch2, file_path2 + \"_output.jsonl\")\n",
    "answer_df2 = get_answer_list(file_path2 + \"_output.jsonl\")\n",
    "download_output(retrieve_batch3, file_path3 + \"_output.jsonl\")\n",
    "answer_df3 = get_answer_list(file_path3 + \"_output.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Processing and Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_research.insert(len(test_research.columns), column='reserve', value=None)\n",
    "#test_research.insert(len(test_research.columns), column='data-rich', value=None)\n",
    "test_research.insert(len(test_research.columns), column='answer1', value=None)\n",
    "test_research.insert(len(test_research.columns), column='answer2', value=None)\n",
    "test_research.insert(len(test_research.columns), column='answer3', value=None)\n",
    "#question_df = pd.DataFrame(columns=['ID', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, row1), (_, row2),(_, row3) in zip(answer_df1.iterrows(),answer_df2.iterrows(), answer_df3.iterrows()):\n",
    "    id = int(row1['ID'])\n",
    "    answer1 = json.loads(row1['Answer'])\n",
    "    answer2 = json.loads(row2['Answer']) \n",
    "    answer3 = json.loads(row3['Answer'])\n",
    "    a1 = []\n",
    "    a2 = []\n",
    "    a3 = []\n",
    "    for a_name in answer1:\n",
    "        a1.append(answer1[a_name])\n",
    "    for a_name in answer2:\n",
    "        a2.append(answer2[a_name])\n",
    "    for a_name in answer3:\n",
    "        a3.append(answer3[a_name])\n",
    "    test_research.loc[id, 'answer1'] = str(a1)\n",
    "    test_research.loc[id, 'answer2'] = str(a2)\n",
    "    test_research.loc[id, 'answer3'] = str(a3)\n",
    "    #print(a1,a2,a3)\n",
    "    if '1' in a1[0].lower() and ('1' in a1[1].lower() or '1' in a1[2].lower() or '1' in a1[3].lower() ) and '0' in a2[0].lower() and '0' in a2[1].lower() and '0' in a3[0].lower() :\n",
    "        test_research.loc[i, 'reserve'] = 'Y' \n",
    "        #print(111)\n",
    "#print(test_research['answer2'])\n",
    "test_research.to_excel('result2(all).xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
